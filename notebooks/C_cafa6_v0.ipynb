{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies & External Tools"
      ],
      "metadata": {
        "id": "7NbMHN8D9tXU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VChBSXL3o6qz",
        "outputId": "2c1260f2-08cb-4209-e1c6-dd4b80b6996b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/3.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hmmseqs/\n",
            "mmseqs/userguide.pdf\n",
            "mmseqs/examples/\n",
            "mmseqs/examples/DB.fasta\n",
            "mmseqs/examples/QUERY.fasta\n",
            "mmseqs/LICENSE.md\n",
            "mmseqs/README.md\n",
            "mmseqs/matrices/\n",
            "mmseqs/matrices/PAM120.out\n",
            "mmseqs/matrices/PAM190.out\n",
            "mmseqs/matrices/PAM130.out\n",
            "mmseqs/matrices/blosum62.out\n",
            "mmseqs/matrices/blosum80.out\n",
            "mmseqs/matrices/blosum95.out\n",
            "mmseqs/matrices/PAM20.out\n",
            "mmseqs/matrices/blosum75.out\n",
            "mmseqs/matrices/blosum100.out\n",
            "mmseqs/matrices/PAM40.out\n",
            "mmseqs/matrices/PAM100.out\n",
            "mmseqs/matrices/VTML200.out\n",
            "mmseqs/matrices/blosum35.out\n",
            "mmseqs/matrices/PAM70.out\n",
            "mmseqs/matrices/PAM170.out\n",
            "mmseqs/matrices/blosum30.out\n",
            "mmseqs/matrices/VTML40.out\n",
            "mmseqs/matrices/blosum55.out\n",
            "mmseqs/matrices/PAM30.out\n",
            "mmseqs/matrices/PAM10.out\n",
            "mmseqs/matrices/blosum65.out\n",
            "mmseqs/matrices/blosum90.out\n",
            "mmseqs/matrices/PAM160.out\n",
            "mmseqs/matrices/PAM60.out\n",
            "mmseqs/matrices/PAM50.out\n",
            "mmseqs/matrices/PAM180.out\n",
            "mmseqs/matrices/VTML160.out\n",
            "mmseqs/matrices/blosum60.out\n",
            "mmseqs/matrices/blosum85.out\n",
            "mmseqs/matrices/VTML10.out\n",
            "mmseqs/matrices/nucleotide.out\n",
            "mmseqs/matrices/VTML80.out\n",
            "mmseqs/matrices/blosum40.out\n",
            "mmseqs/matrices/PAM80.out\n",
            "mmseqs/matrices/PAM110.out\n",
            "mmseqs/matrices/VTML20.out\n",
            "mmseqs/matrices/PAM90.out\n",
            "mmseqs/matrices/blosum45.out\n",
            "mmseqs/matrices/VTML120.out\n",
            "mmseqs/matrices/blosum70.out\n",
            "mmseqs/matrices/PAM150.out\n",
            "mmseqs/matrices/blosum50.out\n",
            "mmseqs/matrices/PAM140.out\n",
            "mmseqs/bin/\n",
            "mmseqs/bin/mmseqs\n",
            "mmseqs/util/\n",
            "mmseqs/util/bash-completion.sh\n",
            "Dependencies and MMseqs2 installed.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Python libraries\n",
        "!pip install -q biopython obonet networkx transformers torch tqdm\n",
        "\n",
        "# 2. Install MMseqs2 (Static binary for Linux)\n",
        "# We need this for the clustering logic in data_splits.py\n",
        "!mkdir -p /content/mmseqs\n",
        "!wget -q https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz -O /content/mmseqs/mmseqs.tar.gz\n",
        "!tar xvfz /content/mmseqs/mmseqs.tar.gz -C /content/mmseqs\n",
        "!ln -sf /content/mmseqs/mmseqs/bin/mmseqs /usr/local/bin/mmseqs\n",
        "\n",
        "print(\"Dependencies and MMseqs2 installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Data Setup (Upload kaggle.json)"
      ],
      "metadata": {
        "id": "_OkyfmG59xwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Upload kaggle.json\n",
        "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print(\"Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 2. Download Competition Data\n",
        "if not os.path.exists('cafa-6-protein-function-prediction.zip'):\n",
        "    print(\"Downloading dataset...\")\n",
        "    !kaggle competitions download -c cafa-6-protein-function-prediction\n",
        "\n",
        "# 3. Organize Directory Structure\n",
        "# Your code expects: data/raw/Train, data/embeddings, data/splits\n",
        "print(\"Organizing directory structure...\")\n",
        "\n",
        "!mkdir -p data/raw\n",
        "!mkdir -p data/embeddings/train\n",
        "!mkdir -p data/splits\n",
        "!mkdir -p results\n",
        "\n",
        "# Unzip and move\n",
        "!unzip -q cafa-6-protein-function-prediction.zip -d temp_data\n",
        "\n",
        "# Move files to match your project tree\n",
        "# Note: Adjusting based on standard Kaggle unzip structure\n",
        "if os.path.exists('temp_data/Train'):\n",
        "    !mv temp_data/Train data/raw/\n",
        "    !mv temp_data/Test data/raw/\n",
        "    !mv temp_data/IA.tsv data/raw/\n",
        "    !mv temp_data/sample_submission.tsv data/raw/\n",
        "else:\n",
        "    # Fallback if zip structure is flat\n",
        "    !mkdir -p data/raw/Train data/raw/Test\n",
        "    !mv temp_data/train_* data/raw/Train/ 2>/dev/null || true\n",
        "    !mv temp_data/go-basic.obo data/raw/Train/\n",
        "    !mv temp_data/testsuperset* data/raw/Test/ 2>/dev/null || true\n",
        "\n",
        "!rm -rf temp_data\n",
        "print(\"Data setup complete. Structure:\")"
      ],
      "metadata": {
        "id": "_oEG3vFq3E-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "4ce722eb-55ed-4b68-b002-0fe1ce491295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd6b69f9-f7db-4a30-917a-66e063cb39fe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fd6b69f9-f7db-4a30-917a-66e063cb39fe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading dataset...\n",
            "Downloading cafa-6-protein-function-prediction.zip to /content\n",
            "  0% 0.00/91.3M [00:00<?, ?B/s]\n",
            "100% 91.3M/91.3M [00:00<00:00, 1.28GB/s]\n",
            "Organizing directory structure...\n",
            "Data setup complete. Structure:\n",
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Source Code modules"
      ],
      "metadata": {
        "id": "0cs9OgmXAeCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('src', exist_ok=True)\n",
        "\n",
        "# Add src to python path for immediate imports if needed\n",
        "import sys\n",
        "sys.path.append('/content/src')"
      ],
      "metadata": {
        "id": "B5SwH3H0AfLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/go_labeler.py\n",
        "\"\"\"\n",
        "GO Labeler: Handles Gene Ontology logic for protein function prediction.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import obonet\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict, Set, Optional\n",
        "\n",
        "class GOLabeler:\n",
        "    def __init__(self, obo_path: str, annotations: List[Tuple[str, str]]):\n",
        "        self.obo_path = obo_path\n",
        "        self.annotations = annotations\n",
        "        print(f\"Loading GO graph from {obo_path}...\")\n",
        "        self.go_graph = obonet.read_obo(obo_path)\n",
        "        print(f\"Loaded GO graph with {self.go_graph.number_of_nodes()} terms.\")\n",
        "\n",
        "        self.term_to_index: Dict[str, int] = {}\n",
        "        self.index_to_term: Dict[int, str] = {}\n",
        "        self.valid_terms: List[str] = []\n",
        "        self._term_frequencies: Dict[str, int] = {}\n",
        "\n",
        "    def _get_ancestors(self, term_id: str) -> Set[str]:\n",
        "        ancestors = set()\n",
        "        if term_id not in self.go_graph:\n",
        "            return ancestors\n",
        "        ancestors.add(term_id)\n",
        "        try:\n",
        "            ancestors.update(nx.descendants(self.go_graph, term_id))\n",
        "        except nx.NetworkXError:\n",
        "            pass\n",
        "        return ancestors\n",
        "\n",
        "    def _propagate_terms(self, terms: List[str]) -> Set[str]:\n",
        "        all_terms = set()\n",
        "        for term in terms:\n",
        "            all_terms.update(self._get_ancestors(term))\n",
        "        return all_terms\n",
        "\n",
        "    def build_label_vocabulary(self, min_frequency: int = 50) -> None:\n",
        "        print(f\"Building label vocabulary with min_frequency={min_frequency}...\")\n",
        "        protein_to_terms: Dict[str, List[str]] = defaultdict(list)\n",
        "        for protein_id, term_id in self.annotations:\n",
        "            protein_to_terms[protein_id].append(term_id)\n",
        "\n",
        "        term_counts: Dict[str, int] = defaultdict(int)\n",
        "        for protein_id, terms in protein_to_terms.items():\n",
        "            propagated_terms = self._propagate_terms(terms)\n",
        "            for term in propagated_terms:\n",
        "                term_counts[term] += 1\n",
        "\n",
        "        self._term_frequencies = dict(term_counts)\n",
        "        self.valid_terms = sorted([\n",
        "            term for term, count in term_counts.items()\n",
        "            if count >= min_frequency\n",
        "        ])\n",
        "\n",
        "        print(f\"Terms meeting min_frequency threshold: {len(self.valid_terms)}\")\n",
        "        self.term_to_index = {term: idx for idx, term in enumerate(self.valid_terms)}\n",
        "        self.index_to_term = {idx: term for idx, term in enumerate(self.valid_terms)}\n",
        "\n",
        "    def get_vector(self, protein_terms: List[str]) -> np.ndarray:\n",
        "        if not self.valid_terms:\n",
        "            raise ValueError(\"Vocabulary not built.\")\n",
        "        vector = np.zeros(len(self.valid_terms), dtype=np.float32)\n",
        "        propagated_terms = self._propagate_terms(protein_terms)\n",
        "        for term in propagated_terms:\n",
        "            if term in self.term_to_index:\n",
        "                idx = self.term_to_index[term]\n",
        "                vector[idx] = 1.0\n",
        "        return vector\n",
        "\n",
        "    def vocabulary_size(self) -> int:\n",
        "        return len(self.valid_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6IRkiHHAo-X",
        "outputId": "f261825b-f02b-42ee-8e71-22c2910ca2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/go_labeler.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/data_splits.py\n",
        "\"\"\"\n",
        "Data Splitting Utilities for Protein Function Prediction.\n",
        "\"\"\"\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "\n",
        "def create_splits(\n",
        "    cluster_file: str,\n",
        "    val_ratio: float = 0.2,\n",
        "    random_seed: Optional[int] = 42\n",
        ") -> Tuple[Set[str], Set[str]]:\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    clusters: Dict[str, List[str]] = defaultdict(list)\n",
        "    print(f\"Reading cluster file: {cluster_file}\")\n",
        "    with open(cluster_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) < 2: continue\n",
        "            cluster_rep, sequence_member = parts[0], parts[1]\n",
        "            clusters[cluster_rep].append(sequence_member)\n",
        "\n",
        "    cluster_reps = list(clusters.keys())\n",
        "    n_clusters = len(cluster_reps)\n",
        "    total_sequences = sum(len(members) for members in clusters.values())\n",
        "    print(f\"Found {n_clusters} clusters containing {total_sequences} sequences\")\n",
        "\n",
        "    random.shuffle(cluster_reps)\n",
        "    n_val_clusters = int(n_clusters * val_ratio)\n",
        "\n",
        "    val_cluster_reps = set(cluster_reps[:n_val_clusters])\n",
        "\n",
        "    train_protein_ids: Set[str] = set()\n",
        "    val_protein_ids: Set[str] = set()\n",
        "\n",
        "    for cluster_rep, members in clusters.items():\n",
        "        if cluster_rep in val_cluster_reps:\n",
        "            val_protein_ids.update(members)\n",
        "        else:\n",
        "            train_protein_ids.update(members)\n",
        "\n",
        "    print(f\"Split complete: Train {len(train_protein_ids)}, Val {len(val_protein_ids)}\")\n",
        "    return train_protein_ids, val_protein_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU3gwx8GA_ZD",
        "outputId": "2ba790ba-6e29-48a3-8039-dd99b75e5a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data_splits.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/dataset.py\n",
        "\"\"\"\n",
        "PyTorch Dataset for Protein Function Prediction.\n",
        "\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class ProteinGODataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        protein_ids: List[str],\n",
        "        labeler,\n",
        "        embedding_dir: str,\n",
        "        annotations: Dict[str, List[str]],\n",
        "        check_exists: bool = True\n",
        "    ):\n",
        "        self.labeler = labeler\n",
        "        self.embedding_dir = Path(embedding_dir)\n",
        "        self.annotations = annotations\n",
        "\n",
        "        if check_exists:\n",
        "            self.protein_ids = []\n",
        "            for pid in protein_ids:\n",
        "                embedding_path = self.embedding_dir / f\"{pid}.pt\"\n",
        "                if embedding_path.exists():\n",
        "                    self.protein_ids.append(pid)\n",
        "        else:\n",
        "            self.protein_ids = list(protein_ids)\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.protein_ids)} proteins\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.protein_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        protein_id = self.protein_ids[idx]\n",
        "        embedding_path = self.embedding_dir / f\"{protein_id}.pt\"\n",
        "        embedding = torch.load(embedding_path, weights_only=True).to(torch.float32)\n",
        "\n",
        "        explicit_terms = self.annotations.get(protein_id, [])\n",
        "        label_numpy = self.labeler.get_vector(explicit_terms)\n",
        "        label = torch.from_numpy(label_numpy).to(torch.float32)\n",
        "\n",
        "        return embedding, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8jBCQEGBEVF",
        "outputId": "bb70825e-f55b-445e-8077-18380dc9c98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/model.py\n",
        "\"\"\"\n",
        "Neural Network Model and Loss Components.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "\n",
        "class ProteinMLP(nn.Module):\n",
        "    def __init__(self, num_classes: int, input_dim: int = 2560, hidden_dim: int = 1024, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_dim, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.hidden(x)\n",
        "        return self.output(x)\n",
        "\n",
        "def calculate_pos_weights(train_dataset, num_workers: int = 0, batch_size: int = 256) -> torch.Tensor:\n",
        "    loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    _, first_label = train_dataset[0]\n",
        "    num_classes = first_label.shape[0]\n",
        "\n",
        "    positive_counts = torch.zeros(num_classes, dtype=torch.float64)\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Calculating positive class weights...\")\n",
        "    for _, labels in tqdm(loader, desc=\"Computing weights\"):\n",
        "        positive_counts += labels.sum(dim=0).to(torch.float64)\n",
        "        total_samples += labels.shape[0]\n",
        "\n",
        "    epsilon = 1e-7\n",
        "    negative_counts = total_samples - positive_counts\n",
        "    pos_weights = negative_counts / (positive_counts + epsilon)\n",
        "    pos_weights = torch.clamp(pos_weights, min=1.0, max=100.0)\n",
        "\n",
        "    return pos_weights.to(torch.float32)\n",
        "\n",
        "def create_loss_function(pos_weights: Optional[torch.Tensor] = None, device: str = \"cuda\") -> nn.BCEWithLogitsLoss:\n",
        "    if pos_weights is not None:\n",
        "        pos_weights = pos_weights.to(device)\n",
        "        return nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
        "    return nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMwO_mIqBGuy",
        "outputId": "7bc7cd0b-b60a-4511-afb5-bd9ad273213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/threshold_optimizer.py\n",
        "\"\"\"\n",
        "Threshold Optimization for Multi-Label Classification.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import autocast\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def collect_predictions(model, data_loader, device, use_amp=True):\n",
        "    model.eval()\n",
        "    use_amp = use_amp and device == \"cuda\"\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in tqdm(data_loader, desc=\"Collecting predictions\"):\n",
        "            embeddings = embeddings.to(device)\n",
        "            with autocast(device_type=device, enabled=use_amp):\n",
        "                logits = model(embeddings)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(labels.numpy())\n",
        "\n",
        "    return np.concatenate(all_probs, axis=0), np.concatenate(all_labels, axis=0)\n",
        "\n",
        "def optimize_thresholds(model, val_loader, device, use_amp=True) -> np.ndarray:\n",
        "    print(\"Optimizing Per-Class Thresholds\")\n",
        "    y_probs, y_true = collect_predictions(model, val_loader, device, use_amp)\n",
        "    n_classes = y_probs.shape[1]\n",
        "    optimal_thresholds = np.full(n_classes, 0.5)\n",
        "\n",
        "    for class_idx in tqdm(range(n_classes), desc=\"Optimizing\"):\n",
        "        y_true_class = y_true[:, class_idx]\n",
        "        if y_true_class.sum() == 0 or y_true_class.sum() == len(y_true_class):\n",
        "            continue\n",
        "\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true_class, y_probs[:, class_idx])\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
        "            f1_scores = np.nan_to_num(f1_scores, nan=0.0)\n",
        "\n",
        "        if len(f1_scores) > 0 and f1_scores.max() > 0:\n",
        "            optimal_thresholds[class_idx] = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "    return optimal_thresholds\n",
        "\n",
        "def evaluate_with_thresholds(model, data_loader, device, thresholds, use_amp=True):\n",
        "    y_probs, y_true = collect_predictions(model, data_loader, device, use_amp)\n",
        "    y_pred = (y_probs >= thresholds).astype(np.float32)\n",
        "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Evaluation - Micro F1: {micro_f1:.4f}, Macro F1: {macro_f1:.4f}\")\n",
        "    return {'micro_f1': micro_f1, 'macro_f1': macro_f1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX0CiJLZBJCt",
        "outputId": "3da34396-0bf7-44d1-d3d0-eeaf677993e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/threshold_optimizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/trainer.py\n",
        "\"\"\"\n",
        "Training Utilities.\n",
        "\"\"\"\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: str,\n",
        "    epochs: int,\n",
        "    patience: int = 5,\n",
        "    checkpoint_dir: str = \"models\",\n",
        "    checkpoint_name: str = \"best_model.pt\",\n",
        "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
        "    use_amp: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
        "    best_model_path = Path(checkpoint_dir) / checkpoint_name\n",
        "\n",
        "    use_amp = use_amp and device == \"cuda\"\n",
        "    scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "    stopped_early = False\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\", leave=False):\n",
        "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type=device, enabled=use_amp):\n",
        "                logits = model(embeddings)\n",
        "                loss = loss_fn(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss_sum += loss.item()\n",
        "            batches += 1\n",
        "\n",
        "        avg_train_loss = train_loss_sum / batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for embeddings, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\", leave=False):\n",
        "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "                with autocast(device_type=device, enabled=use_amp):\n",
        "                    logits = model(embeddings)\n",
        "                    loss = loss_fn(logits, labels)\n",
        "                val_loss_sum += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss_sum / val_batches\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        if scheduler:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(avg_val_loss)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'val_loss': best_val_loss\n",
        "            }, best_model_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping\")\n",
        "                stopped_early = True\n",
        "                break\n",
        "\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return {'train_losses': train_losses, 'val_losses': val_losses, 'best_epoch': best_epoch, 'best_val_loss': best_val_loss}\n",
        "\n",
        "def load_checkpoint(model, path, device=\"cpu\"):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qdCyY0BBNYd",
        "outputId": "ebd7d424-b656-4ce8-be84-849e9e77a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/extract_embeddings.py\n",
        "import os\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from Bio import SeqIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1022\n",
        "\n",
        "def process_sequence(sequence, model, tokenizer, device):\n",
        "    if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
        "        sequence = sequence[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=False, truncation=True, max_length=MAX_SEQUENCE_LENGTH + 2)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Mean pooling (excluding CLS/EOS)\n",
        "    return outputs.last_hidden_state[:, 1:-1, :].mean(dim=1).squeeze(0).cpu()\n",
        "\n",
        "def process_fasta(fasta_path, output_dir, model_name, device):\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
        "\n",
        "    print(f\"Extracting embeddings using {model_name}\")\n",
        "\n",
        "    total = sum(1 for _ in SeqIO.parse(fasta_path, \"fasta\"))\n",
        "\n",
        "    with tqdm(total=total) as pbar:\n",
        "        for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
        "            out_file = output_path / f\"{record.id}.pt\"\n",
        "            if not out_file.exists():\n",
        "                try:\n",
        "                    emb = process_sequence(str(record.seq), model, tokenizer, device)\n",
        "                    torch.save(emb, out_file)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error {record.id}: {e}\")\n",
        "            pbar.update(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--fasta\", required=True)\n",
        "    parser.add_argument(\"--output\", required=True)\n",
        "    parser.add_argument(\"--model\", default=\"facebook/esm2_t36_3B_UR50D\")\n",
        "    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    process_fasta(args.fasta, args.output, args.model, args.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcB4V_s4BRiO",
        "outputId": "db619eb2-6e59-4443-bc29-c63927e3593f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/main.py\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from go_labeler import GOLabeler\n",
        "from data_splits import create_splits\n",
        "from dataset import ProteinGODataset\n",
        "from model import ProteinMLP, calculate_pos_weights, create_loss_function\n",
        "from trainer import train_model, load_checkpoint\n",
        "from threshold_optimizer import optimize_thresholds, evaluate_with_thresholds\n",
        "\n",
        "def load_annotations(path):\n",
        "    ann_list, ann_dict = [], defaultdict(list)\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                ann_list.append((parts[0], parts[1]))\n",
        "                ann_dict[parts[0]].append(parts[1])\n",
        "    return ann_list, dict(ann_dict)\n",
        "\n",
        "def main(args):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Running on {device}\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    annotations_list, annotations_dict = load_annotations(args.annotations_path)\n",
        "    labeler = GOLabeler(args.obo_path, annotations_list)\n",
        "    labeler.build_label_vocabulary(min_frequency=args.min_frequency)\n",
        "\n",
        "    # 2. Splits\n",
        "    train_ids, val_ids = create_splits(args.cluster_path, args.val_ratio, args.seed)\n",
        "    train_ids = train_ids & set(annotations_dict.keys())\n",
        "    val_ids = val_ids & set(annotations_dict.keys())\n",
        "\n",
        "    # 3. Datasets\n",
        "    train_dataset = ProteinGODataset(list(train_ids), labeler, args.embedding_dir, annotations_dict)\n",
        "    val_dataset = ProteinGODataset(list(val_ids), labeler, args.embedding_dir, annotations_dict)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    # 4. Model Setup\n",
        "    pos_weights = calculate_pos_weights(train_dataset) if args.use_pos_weights else None\n",
        "    model = ProteinMLP(num_classes=labeler.vocabulary_size(), input_dim=args.input_dim, hidden_dim=args.hidden_dim, dropout=args.dropout)\n",
        "    loss_fn = create_loss_function(pos_weights, device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # 5. Train\n",
        "    train_model(model, train_loader, val_loader, loss_fn, optimizer, device, args.epochs, args.patience, args.output_dir, \"best_model.pt\", scheduler)\n",
        "\n",
        "    # 6. Optimization\n",
        "    print(\"Loading best model for optimization...\")\n",
        "    model, _ = load_checkpoint(model, str(Path(args.output_dir)/\"best_model.pt\"), device)\n",
        "    thresholds = optimize_thresholds(model, val_loader, device)\n",
        "    evaluate_with_thresholds(model, val_loader, device, thresholds)\n",
        "\n",
        "    # Save artifacts\n",
        "    np.save(Path(args.output_dir)/\"thresholds.npy\", thresholds)\n",
        "    np.savez(Path(args.output_dir)/\"vocabulary.npz\", valid_terms=np.array(labeler.valid_terms))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--obo_path\", default=\"data/raw/Train/go-basic.obo\")\n",
        "    parser.add_argument(\"--annotations_path\", default=\"data/raw/Train/train_terms.tsv\")\n",
        "    parser.add_argument(\"--cluster_path\", default=\"data/splits/train_cluster.tsv\")\n",
        "    parser.add_argument(\"--embedding_dir\", default=\"data/embeddings/train\")\n",
        "    parser.add_argument(\"--output_dir\", default=\"results/experiment_001\")\n",
        "    parser.add_argument(\"--input_dim\", type=int, default=1280) # Changed default to match esm2_t33_650M\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=1024)\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=256)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
        "    parser.add_argument(\"--patience\", type=int, default=5)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
        "    parser.add_argument(\"--min_frequency\", type=int, default=50)\n",
        "    parser.add_argument(\"--val_ratio\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--use_pos_weights\", action=\"store_true\")\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=2)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDUJJL5rBUOp",
        "outputId": "97b698cd-9e45-45fa-ac86-b3add3e1f36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing - Clustering"
      ],
      "metadata": {
        "id": "h-6K3LUvBbBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate clusters (30% identity)\n",
        "# This creates data/splits/train_cluster.tsv\n",
        "!mmseqs easy-linclust \\\n",
        "    data/raw/Train/train_sequences.fasta \\\n",
        "    data/splits/train \\\n",
        "    tmp \\\n",
        "    --min-seq-id 0.3 \\\n",
        "    --cov-mode 1 \\\n",
        "    -c 0.8\n",
        "\n",
        "# Rename output to match what src/data_splits.py expects\n",
        "!mv data/splits/train_cluster.tsv data/splits/train_cluster.tsv.bak\n",
        "# Filter out headers if any and ensure tab separated\n",
        "!awk '{print $1\"\\t\"$2}' data/splits/train_cluster.tsv.bak > data/splits/train_cluster.tsv\n",
        "!head -n 5 data/splits/train_cluster.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMO1vrwdBb6f",
        "outputId": "21288406-9e54-4697-9226-8e41c5dbd866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create directory tmp\n",
            "easy-linclust data/raw/Train/train_sequences.fasta data/splits/train tmp --min-seq-id 0.3 --cov-mode 1 -c 0.8 \n",
            "\n",
            "MMseqs Version:                     \tbd01c2229f027d8d8e61947f44d11ef1a7669212\n",
            "Cluster mode                        \t0\n",
            "Max connected component depth       \t1000\n",
            "Similarity type                     \t2\n",
            "Threads                             \t2\n",
            "Compressed                          \t0\n",
            "Verbosity                           \t3\n",
            "Weight file name                    \t\n",
            "Cluster Weight threshold            \t0.9\n",
            "Set mode                            \tfalse\n",
            "Substitution matrix                 \taa:blosum62.out,nucl:nucleotide.out\n",
            "Add backtrace                       \tfalse\n",
            "Alignment mode                      \t0\n",
            "Alignment mode                      \t0\n",
            "Allow wrapped scoring               \tfalse\n",
            "E-value threshold                   \t0.001\n",
            "Seq. id. threshold                  \t0.3\n",
            "Min alignment length                \t0\n",
            "Seq. id. mode                       \t0\n",
            "Alternative alignments              \t0\n",
            "Coverage threshold                  \t0.8\n",
            "Coverage mode                       \t1\n",
            "Max sequence length                 \t65535\n",
            "Compositional bias                  \t1\n",
            "Compositional bias scale            \t1\n",
            "Max reject                          \t2147483647\n",
            "Max accept                          \t2147483647\n",
            "Include identical seq. id.          \tfalse\n",
            "Preload mode                        \t0\n",
            "Pseudo count a                      \tsubstitution:1.100,context:1.400\n",
            "Pseudo count b                      \tsubstitution:4.100,context:5.800\n",
            "Score bias                          \t0\n",
            "Realign hits                        \tfalse\n",
            "Realign score bias                  \t-0.2\n",
            "Realign max seqs                    \t2147483647\n",
            "Correlation score weight            \t0\n",
            "Gap open cost                       \taa:11,nucl:5\n",
            "Gap extension cost                  \taa:1,nucl:2\n",
            "Zdrop                               \t40\n",
            "Alphabet size                       \taa:21,nucl:5\n",
            "k-mers per sequence                 \t21\n",
            "Spaced k-mers                       \t0\n",
            "Spaced k-mer pattern                \t\n",
            "Scale k-mers per sequence           \taa:0.000,nucl:0.200\n",
            "Adjust k-mer length                 \tfalse\n",
            "Mask residues                       \t1\n",
            "Mask residues probability           \t0.9\n",
            "Mask lower case residues            \t0\n",
            "Mask lower letter repeating N times \t0\n",
            "k-mer length                        \t0\n",
            "Shift hash                          \t67\n",
            "Split memory limit                  \t0\n",
            "Include only extendable             \tfalse\n",
            "Skip repeating k-mers               \tfalse\n",
            "Rescore mode                        \t0\n",
            "Remove hits by seq. id. and coverage\tfalse\n",
            "Sort results                        \t0\n",
            "Remove temporary files              \ttrue\n",
            "Force restart with latest tmp       \tfalse\n",
            "MPI runner                          \t\n",
            "Database type                       \t0\n",
            "Shuffle input database              \ttrue\n",
            "Createdb mode                       \t1\n",
            "Write lookup file                   \t0\n",
            "Offset of numeric ids               \t0\n",
            "Use GPU                             \t0\n",
            "\n",
            "createdb data/raw/Train/train_sequences.fasta tmp/13312381584335532710/input --createdb-mode 1 --write-lookup 0 \n",
            "\n",
            "\u001b[33mShuffle database cannot be combined with --createdb-mode 1\n",
            "\u001b[39m\u001b[33mWe recompute with --shuffle 0\n",
            "\u001b[39mConverting sequences\n",
            "\u001b[33mMultiline fasta can not be combined with --createdb-mode 0\n",
            "\u001b[39m\u001b[33mWe recompute with --createdb-mode 1\n",
            "\u001b[39mTime for merging to input_h: 0h 0m 0s 0ms\n",
            "Time for merging to input: 0h 0m 0s 0ms\n",
            "[82316] 0s 251ms\n",
            "Time for merging to input_h: 0h 0m 0s 0ms\n",
            "Time for merging to input: 0h 0m 0s 20ms\n",
            "Database type: Aminoacid\n",
            "Time for processing: 0h 0m 0s 290ms\n",
            "Create directory tmp/13312381584335532710/clu_tmp\n",
            "linclust tmp/13312381584335532710/input tmp/13312381584335532710/clu tmp/13312381584335532710/clu_tmp -e 0.001 --min-seq-id 0.3 -c 0.8 --cov-mode 1 --spaced-kmer-mode 0 --remove-tmp-files 1 \n",
            "\n",
            "Set cluster mode GREEDY MEM.\n",
            "kmermatcher tmp/13312381584335532710/input tmp/13312381584335532710/clu_tmp/3470343162581046629/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.3 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --mask-n-repeat 0 --cov-mode 1 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 2 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
            "\n",
            "kmermatcher tmp/13312381584335532710/input tmp/13312381584335532710/clu_tmp/3470343162581046629/pref --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --alph-size aa:13,nucl:5 --min-seq-id 0.3 --kmer-per-seq 21 --spaced-kmer-mode 0 --kmer-per-seq-scale aa:0.000,nucl:0.200 --adjust-kmer-len 0 --mask 0 --mask-prob 0.9 --mask-lower-case 0 --mask-n-repeat 0 --cov-mode 1 -k 0 -c 0.8 --max-seq-len 65535 --hash-shift 67 --split-memory-limit 0 --include-only-extendable 0 --ignore-multi-kmer 0 --threads 2 --compressed 0 -v 3 --cluster-weight-threshold 0.9 \n",
            "\n",
            "Database size: 82404 type: Aminoacid\n",
            "Reduced amino acid alphabet: (A S T) (C) (D B N) (E Q Z) (F Y) (G) (H) (I V) (K R) (L J M) (P) (W) (X) \n",
            "\n",
            "Generate k-mers list for 1 split\n",
            "[=================================================================] 100.00% 82.40K 1s 875ms\n",
            "Sort kmer 0h 0m 0s 560ms\n",
            "Sort by rep. sequence 0h 0m 0s 271ms\n",
            "Time for fill: 0h 0m 0s 55ms\n",
            "Time for merging to pref: 0h 0m 0s 0ms\n",
            "Time for processing: 0h 0m 2s 960ms\n",
            "rescorediagonal tmp/13312381584335532710/input tmp/13312381584335532710/input tmp/13312381584335532710/clu_tmp/3470343162581046629/pref tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore1 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 0 --wrapped-scoring 0 --filter-hits 0 -e 0.001 -c 0.8 -a 0 --cov-mode 1 --min-seq-id 0.5 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 2 --compressed 0 -v 3 \n",
            "\n",
            "[=================================================================] 100.00% 82.40K 0s 306ms\n",
            "Time for merging to pref_rescore1: 0h 0m 0s 77ms\n",
            "Time for processing: 0h 0m 0s 559ms\n",
            "clust tmp/13312381584335532710/input tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore1 tmp/13312381584335532710/clu_tmp/3470343162581046629/pre_clust --cluster-mode 3 --max-iterations 1000 --similarity-type 2 --threads 2 --compressed 0 -v 3 --cluster-weight-threshold 0.9 --set-mode 0 \n",
            "\n",
            "Clustering mode: Greedy Low Mem\n",
            "Total time: 0h 0m 0s 245ms\n",
            "\n",
            "Size of the sequence database: 82404\n",
            "Size of the alignment database: 82404\n",
            "Number of clusters: 57846\n",
            "\n",
            "Writing results 0h 0m 0s 52ms\n",
            "Time for merging to pre_clust: 0h 0m 0s 0ms\n",
            "Time for processing: 0h 0m 0s 436ms\n",
            "createsubdb tmp/13312381584335532710/clu_tmp/3470343162581046629/order_redundancy tmp/13312381584335532710/input tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy -v 3 --subdb-mode 1 \n",
            "\n",
            "Time for merging to input_step_redundancy: 0h 0m 0s 0ms\n",
            "Time for processing: 0h 0m 0s 56ms\n",
            "createsubdb tmp/13312381584335532710/clu_tmp/3470343162581046629/order_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/pref tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter1 -v 3 --subdb-mode 1 \n",
            "\n",
            "Time for merging to pref_filter1: 0h 0m 0s 0ms\n",
            "Time for processing: 0h 0m 0s 112ms\n",
            "filterdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter1 tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter2 --filter-file tmp/13312381584335532710/clu_tmp/3470343162581046629/order_redundancy --threads 2 --compressed 0 -v 3 \n",
            "\n",
            "Filtering using file(s)\n",
            "[=================================================================] 100.00% 57.85K 0s 114ms\n",
            "Time for merging to pref_filter2: 0h 0m 0s 84ms\n",
            "Time for processing: 0h 0m 0s 314ms\n",
            "rescorediagonal tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter2 tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore2 --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' --rescore-mode 1 --wrapped-scoring 0 --filter-hits 1 -e 0.001 -c 0.8 -a 0 --cov-mode 1 --min-seq-id 0.3 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --db-load-mode 0 --threads 2 --compressed 0 -v 3 \n",
            "\n",
            "[=================================================================] 100.00% 57.85K 0s 273ms\n",
            "Time for merging to pref_rescore2: 0h 0m 0s 15ms\n",
            "Time for processing: 0h 0m 0s 360ms\n",
            "align tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore2 tmp/13312381584335532710/clu_tmp/3470343162581046629/aln --sub-mat 'aa:blosum62.out,nucl:nucleotide.out' -a 0 --alignment-mode 2 --alignment-output-mode 0 --wrapped-scoring 0 -e 0.001 --min-seq-id 0.3 --min-aln-len 0 --seq-id-mode 0 --alt-ali 0 -c 0.8 --cov-mode 1 --max-seq-len 65535 --comp-bias-corr 1 --comp-bias-corr-scale 1 --max-rejected 2147483647 --max-accept 2147483647 --add-self-matches 0 --db-load-mode 0 --pca substitution:1.100,context:1.400 --pcb substitution:4.100,context:5.800 --score-bias 0 --realign 0 --realign-score-bias -0.2 --realign-max-seqs 2147483647 --corr-score-weight 0 --gap-open aa:11,nucl:5 --gap-extend aa:1,nucl:2 --zdrop 40 --threads 2 --compressed 0 -v 3 \n",
            "\n",
            "Compute score and coverage\n",
            "Query database size: 57846 type: Aminoacid\n",
            "Target database size: 57846 type: Aminoacid\n",
            "Calculation of alignments\n",
            "[=================================================================] 100.00% 57.85K 38s 609ms\n",
            "Time for merging to aln: 0h 0m 0s 99ms\n",
            "86017 alignments calculated\n",
            "76072 sequence pairs passed the thresholds (0.884383 of overall calculated)\n",
            "1.315078 hits per query sequence\n",
            "Time for processing: 0h 0m 38s 789ms\n",
            "clust tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy tmp/13312381584335532710/clu_tmp/3470343162581046629/aln tmp/13312381584335532710/clu_tmp/3470343162581046629/clust --cluster-mode 3 --max-iterations 1000 --similarity-type 2 --threads 2 --compressed 0 -v 3 --cluster-weight-threshold 0.9 --set-mode 0 \n",
            "\n",
            "Clustering mode: Greedy Low Mem\n",
            "Total time: 0h 0m 0s 273ms\n",
            "\n",
            "Size of the sequence database: 57846\n",
            "Size of the alignment database: 57846\n",
            "Number of clusters: 45294\n",
            "\n",
            "Writing results 0h 0m 0s 52ms\n",
            "Time for merging to clust: 0h 0m 0s 6ms\n",
            "Time for processing: 0h 0m 0s 449ms\n",
            "mergeclusters tmp/13312381584335532710/input tmp/13312381584335532710/clu tmp/13312381584335532710/clu_tmp/3470343162581046629/pre_clust tmp/13312381584335532710/clu_tmp/3470343162581046629/clust --threads 2 --compressed 0 -v 3 \n",
            "\n",
            "Clustering step 1\n",
            "[=================================================================] 100.00% 57.85K 0s 78ms\n",
            "Clustering step 2\n",
            "[=================================================================] 100.00% 45.29K 0s 147ms\n",
            "Write merged clustering\n",
            "[=================================================================] 100.00% 82.40K 0s 230ms\n",
            "Time for merging to clu: 0h 0m 0s 36ms\n",
            "Time for processing: 0h 0m 0s 337ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter1 -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 1ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 0ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore1 -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 2ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pre_clust -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 0ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 2ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/input_step_redundancy_h -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 0ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_filter2 -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 0ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/pref_rescore2 -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 1ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/aln -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 2ms\n",
            "rmdb tmp/13312381584335532710/clu_tmp/3470343162581046629/clust -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 3ms\n",
            "createtsv tmp/13312381584335532710/input tmp/13312381584335532710/input tmp/13312381584335532710/clu tmp/13312381584335532710/cluster.tsv --threads 2 -v 3 \n",
            "\n",
            "Time for merging to cluster.tsv: 0h 0m 0s 39ms\n",
            "Time for processing: 0h 0m 0s 263ms\n",
            "result2repseq tmp/13312381584335532710/input tmp/13312381584335532710/clu tmp/13312381584335532710/clu_rep --db-load-mode 0 --compressed 0 --threads 2 -v 3 \n",
            "\n",
            "[=================================================================] 100.00% 45.29K 0s 60ms\n",
            "Time for merging to clu_rep: 0h 0m 0s 209ms\n",
            "Time for processing: 0h 0m 0s 381ms\n",
            "result2flat tmp/13312381584335532710/input tmp/13312381584335532710/input tmp/13312381584335532710/clu_rep tmp/13312381584335532710/rep_seq.fasta --use-fasta-header -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 296ms\n",
            "createseqfiledb tmp/13312381584335532710/input tmp/13312381584335532710/clu tmp/13312381584335532710/clu_seqs --threads 2 -v 3 \n",
            "\n",
            "[=================================================================] 100.00% 45.29K 0s 114ms\n",
            "Time for merging to clu_seqs: 0h 0m 0s 93ms\n",
            "Time for processing: 0h 0m 0s 319ms\n",
            "result2flat tmp/13312381584335532710/input tmp/13312381584335532710/input tmp/13312381584335532710/clu_seqs tmp/13312381584335532710/all_seqs.fasta -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 380ms\n",
            "rmdb tmp/13312381584335532710/input -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 11ms\n",
            "rmdb tmp/13312381584335532710/input_h -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 1ms\n",
            "rmdb tmp/13312381584335532710/clu_seqs -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 21ms\n",
            "rmdb tmp/13312381584335532710/clu_rep -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 3ms\n",
            "rmdb tmp/13312381584335532710/clu -v 3 \n",
            "\n",
            "Time for processing: 0h 0m 0s 0ms\n",
            "A0A0C5B5G6\tA0A0C5B5G6\n",
            "A0JNW5\tA0JNW5\n",
            "A0PK11\tA0PK11\n",
            "A0PK11\tB2RVW2\n",
            "A1L190\tA1L190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing - Embedding Extraction"
      ],
      "metadata": {
        "id": "UoZ3HKtSBzbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run embedding extraction\n",
        "# This takes time! (Approx 3-5 hours for full CAFA train set on T4)\n",
        "!python src/extract_embeddings.py \\\n",
        "    --fasta data/raw/Train/train_sequences.fasta \\\n",
        "    --output data/embeddings/train \\\n",
        "    --model facebook/esm2_t36_3B_UR50D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-icyWGWBzHD",
        "outputId": "db097fed-4fb8-4efd-d60c-1837b665eedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 95.0/95.0 [00:00<00:00, 664kB/s]\n",
            "vocab.txt: 100% 93.0/93.0 [00:00<00:00, 881kB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 1.44MB/s]\n",
            "config.json: 100% 779/779 [00:00<00:00, 6.61MB/s]\n",
            "2026-01-21 05:37:24.293847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768973844.313478    1289 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768973844.319338    1289 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768973844.334255    1289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768973844.334279    1289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768973844.334283    1289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768973844.334288    1289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 05:37:24.338768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "pytorch_model.bin.index.json: 55.5kB [00:00, 137MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/1.39G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.safetensors.index.json: 58.0kB [00:00, 143MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 475k/9.98G [00:01<9:19:43, 297kB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 1.97M/1.39G [00:01<20:12, 1.15MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 5.16M/1.39G [00:01<06:38, 3.48MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   1% 18.7M/1.39G [00:02<02:07, 10.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   3% 43.4M/1.39G [00:02<00:47, 28.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   4% 58.0M/1.39G [00:03<00:41, 32.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   7% 92.6M/1.39G [00:03<00:30, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  11% 151M/1.39G [00:03<00:16, 76.1MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  15% 203M/1.39G [00:04<00:11, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  18% 245M/1.39G [00:04<00:08, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  20% 272M/1.39G [00:04<00:08, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  23% 323M/1.39G [00:04<00:07, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  25% 349M/1.39G [00:05<00:08, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  27% 373M/1.39G [00:05<00:10, 98.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  28% 390M/1.39G [00:06<00:14, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  31% 431M/1.39G [00:06<00:11, 85.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 1.90M/9.98G [00:06<9:48:13, 283kB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  34% 474M/1.39G [00:06<00:09, 94.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  36% 498M/1.39G [00:06<00:08, 103MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 69.0M/9.98G [00:08<14:17, 11.6MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 136M/9.98G [00:10<09:05, 18.0MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  40% 562M/1.39G [00:10<00:25, 32.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 203M/9.98G [00:10<05:08, 31.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  45% 629M/1.39G [00:10<00:15, 50.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 270M/9.98G [00:10<03:23, 47.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  50% 696M/1.39G [00:11<00:10, 66.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 337M/9.98G [00:11<02:38, 60.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  54% 758M/1.39G [00:11<00:07, 82.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  60% 833M/1.39G [00:12<00:05, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  65% 900M/1.39G [00:12<00:03, 128MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 404M/9.98G [00:14<04:10, 38.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  70% 967M/1.39G [00:14<00:06, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  73% 1.02G/1.39G [00:14<00:04, 76.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 471M/9.98G [00:15<03:07, 50.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 539M/9.98G [00:15<02:11, 71.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 606M/9.98G [00:15<01:38, 94.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  78% 1.09G/1.39G [00:15<00:04, 71.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 673M/9.98G [00:16<01:36, 95.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 740M/9.98G [00:16<01:12, 128MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.98G [00:16<01:06, 138MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  82% 1.14G/1.39G [00:16<00:03, 72.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 874M/9.98G [00:18<02:15, 67.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  86% 1.19G/1.39G [00:18<00:04, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  90% 1.26G/1.39G [00:19<00:02, 59.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 943M/9.98G [00:24<05:39, 26.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  95% 1.32G/1.39G [00:25<00:02, 24.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.98G [00:25<04:42, 31.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.08G/9.98G [00:28<05:13, 28.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.14G/9.98G [00:29<03:48, 38.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:29<02:59, 48.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:30<02:14, 64.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.98G [00:30<01:43, 83.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.98G [00:30<01:23, 103MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.98G [00:30<01:10, 121MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.55G/9.98G [00:31<01:20, 105MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.62G/9.98G [00:32<01:14, 112MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.68G/9.98G [00:34<02:17, 60.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.98G [00:35<01:54, 71.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.98G [00:35<01:27, 93.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.98G [00:39<03:20, 40.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.98G [00:39<02:23, 55.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/9.98G [00:39<01:50, 72.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/9.98G [00:40<01:40, 78.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.98G [00:40<00:56, 137MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.98G [00:40<00:50, 153MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:  95% 1.32G/1.39G [00:40<00:02, 24.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:41<00:48, 159MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00002-of-00002.bin: 100% 1.39G/1.39G [00:41<00:00, 33.8MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.98G [00:41<00:48, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.98G [00:41<00:41, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.98G [00:41<00:38, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/9.98G [00:42<00:34, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:45<02:01, 59.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:45<01:28, 81.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:45<01:13, 97.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.98G [00:46<01:00, 117MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.96G/9.98G [00:46<01:07, 104MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/9.98G [00:47<00:55, 126MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.98G [00:49<01:43, 66.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.98G [00:51<02:17, 49.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.98G [00:51<01:38, 68.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:51<01:14, 90.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.36G/9.98G [00:51<00:57, 114MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.43G/9.98G [00:52<00:50, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.49G/9.98G [00:55<02:09, 50.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.56G/9.98G [00:55<01:34, 68.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.98G [00:56<01:25, 74.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.98G [00:59<02:28, 42.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [01:00<01:37, 63.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.90G/9.98G [01:00<01:17, 78.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.96G/9.98G [01:01<01:18, 77.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.03G/9.98G [01:01<01:01, 96.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.98G [01:03<01:20, 73.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.98G [01:05<01:59, 48.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.98G [01:06<01:33, 61.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [01:07<01:24, 67.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.37G/9.98G [01:08<01:36, 58.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.43G/9.98G [01:10<01:45, 52.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.50G/9.98G [01:10<01:22, 66.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.56G/9.98G [01:11<01:16, 71.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/9.98G [01:12<01:10, 75.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.98G [01:12<01:07, 77.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [01:14<01:30, 57.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.83G/9.98G [01:16<01:33, 55.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [01:16<01:08, 74.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.98G [01:16<00:54, 92.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/9.98G [01:16<00:45, 109MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.10G/9.98G [01:17<00:39, 124MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.17G/9.98G [01:19<01:22, 58.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.23G/9.98G [01:20<01:02, 76.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/9.98G [01:20<00:51, 91.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.37G/9.98G [01:21<00:50, 91.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [01:21<00:30, 148MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.98G [01:21<00:25, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.98G [01:21<00:22, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.71G/9.98G [01:22<00:27, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.98G [01:23<00:27, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.98G [01:23<00:23, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.98G [01:23<00:24, 169MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.98G [01:24<00:27, 143MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.98G [01:28<01:29, 44.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.11G/9.98G [01:28<01:07, 57.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.18G/9.98G [01:29<00:52, 72.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.24G/9.98G [01:29<00:41, 89.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [01:29<00:32, 113MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.98G [01:29<00:26, 138MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.98G [01:30<00:23, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.98G [01:30<00:21, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.58G/9.98G [01:31<00:36, 94.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.98G [01:34<00:58, 57.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.71G/9.98G [01:34<00:42, 76.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.78G/9.98G [01:34<00:33, 95.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.85G/9.98G [01:35<00:27, 113MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.91G/9.98G [01:35<00:23, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.98G [01:35<00:21, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.98G [01:35<00:17, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.98G [01:36<00:17, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.18G/9.98G [01:36<00:15, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.25G/9.98G [01:40<00:58, 46.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [01:41<00:34, 75.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.45G/9.98G [01:46<01:18, 32.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.98G [01:47<00:47, 50.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.65G/9.98G [01:52<01:19, 29.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.72G/9.98G [01:53<00:59, 38.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.79G/9.98G [01:53<00:44, 48.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.98G [01:53<00:33, 63.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.93G/9.98G [01:56<00:43, 47.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.00G/9.98G [01:56<00:32, 60.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.06G/9.98G [01:56<00:26, 72.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.13G/9.98G [01:56<00:19, 95.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [01:57<00:15, 118MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.98G [01:57<00:12, 138MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.33G/9.98G [01:57<00:10, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.40G/9.98G [01:57<00:08, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.98G [01:58<00:08, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.53G/9.98G [02:03<00:36, 39.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.98G [02:03<00:24, 55.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [02:03<00:17, 74.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [02:07<00:33, 37.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [02:07<00:23, 49.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.98G [02:08<00:20, 54.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.91G/9.98G [02:08<00:13, 77.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.98G/9.98G [02:08<00:11, 90.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.98G [02:09<00:08, 107MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [02:09<00:06, 143MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [02:09<00:04, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.98G [02:09<00:03, 186MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.31G/9.98G [02:09<00:03, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.38G/9.98G [02:10<00:02, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.98G [02:10<00:02, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.51G/9.98G [02:10<00:01, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.57G/9.98G [02:11<00:01, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.98G [02:13<00:04, 79.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.71G/9.98G [02:13<00:02, 104MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.98G [02:13<00:01, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.84G/9.98G [02:13<00:00, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.91G/9.98G [02:14<00:00, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [02:14<00:00, 74.2MB/s]\n",
            "Fetching 2 files: 100% 2/2 [02:14<00:00, 67.31s/it] \n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  3.18it/s]\n",
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Extracting embeddings using facebook/esm2_t36_3B_UR50D\n",
            " 22% 18134/82404 [5:22:49<24:01:01,  1.35s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "diS9TaQcF7jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V9wTqMeY1oSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/main.py \\\n",
        "    --obo_path data/raw/Train/go-basic.obo \\\n",
        "    --annotations_path data/raw/Train/train_terms.tsv \\\n",
        "    --cluster_path data/splits/train_cluster.tsv \\\n",
        "    --embedding_dir data/embeddings/train \\\n",
        "    --output_dir results/final_run \\\n",
        "    --input_dim 2560 \\\n",
        "    --batch_size 128 \\\n",
        "    --epochs 20 \\\n",
        "    --use_pos_weights"
      ],
      "metadata": {
        "id": "-eQydQgAF6u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}